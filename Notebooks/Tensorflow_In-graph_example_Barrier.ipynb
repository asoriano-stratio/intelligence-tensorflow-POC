{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow in-graph replication example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gist.github.com/manuzhang/48fa9fe6de8bb9470f0b7092186a74b8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of barrier implementation using TensorFlow shared variables.\n",
    "\n",
    "All workers synchronize on barrier, copy global parameters to local versions\n",
    "and increment global parameter variable asynchronously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import subprocess\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import threading\n",
    "import time\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'ps': [   'test-tf-cluster-ps-0.marathon.l4lb.thisdcos.directory:2333',\n",
      "              'test-tf-cluster-ps-1.marathon.l4lb.thisdcos.directory:2333'],\n",
      "    'worker': [   'test-tf-cluster-worker-0.marathon.l4lb.thisdcos.directory:2333',\n",
      "                  'test-tf-cluster-worker-1.marathon.l4lb.thisdcos.directory:2333',\n",
      "                  'test-tf-cluster-worker-2.marathon.l4lb.thisdcos.directory:2333']}\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "#  => Cluster definition\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "cluster = tf.train.ClusterSpec({\n",
    "    \"worker\": [\n",
    "        \"test-tf-cluster-worker-0.marathon.l4lb.thisdcos.directory:2333\",\n",
    "        \"test-tf-cluster-worker-1.marathon.l4lb.thisdcos.directory:2333\",\n",
    "        \"test-tf-cluster-worker-2.marathon.l4lb.thisdcos.directory:2333\"\n",
    "    ],\n",
    "    \"ps\":[\n",
    "        \"test-tf-cluster-ps-0.marathon.l4lb.thisdcos.directory:2333\",\n",
    "        \"test-tf-cluster-ps-1.marathon.l4lb.thisdcos.directory:2333\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "test_tf_cluster = cluster.as_dict()\n",
    "\n",
    "worker_hosts = test_tf_cluster[\"worker\"]\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint( test_tf_cluster )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAGS = tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.platform.flags._FlagValues object at 0x7f24b3878b70>\n"
     ]
    }
   ],
   "source": [
    "tf.app.flags.DEFINE_integer(\"iters\", 10, \"Maximum number of steps\")\n",
    "tf.app.flags.DEFINE_string(\"wk\", worker_hosts, \"worker hosts\")\n",
    "tf.app.flags.DEFINE_float(\"sleep_interval\", 0.1, \"how long to sleep in wait loop\")\n",
    "\n",
    "print(FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_workers = len(worker_hosts)\n",
    "\n",
    "# global ops\n",
    "init_op = None\n",
    "train_ops = []             # worker local train ops, read local params, update global\n",
    "counter_vars = []          # counters for barrier\n",
    "counter_adder_ops = []\n",
    "global_param_var = None\n",
    "local_param_vars = []\n",
    "local_param_sync_ops = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def default_config():\n",
    "    optimizer_options = tf.OptimizerOptions( opt_level=tf.OptimizerOptions.L0 )\n",
    "    config = tf.ConfigProto( graph_options=tf.GraphOptions(optimizer_options=optimizer_options) )\n",
    "    config.log_device_placement = False\n",
    "    config.allow_soft_placement = False\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_graph(devices):\n",
    "    \"\"\"Create graph that keeps global params + counters on devices[0] and\n",
    "    local params/train ops on devices[:]\"\"\"\n",
    "\n",
    "    global train_ops, counter_vars, counter_adder_ops, global_param_var, local_param_vars, local_param_sync_ops\n",
    "\n",
    "    dtype=tf.int32\n",
    "\n",
    "    with tf.device(devices[0]):\n",
    "        \n",
    "        global_param_var = tf.get_variable(\"param\", shape=(), dtype=dtype, initializer=tf.zeros_initializer)\n",
    "        \n",
    "        for i in range(2):\n",
    "            # tf.get_variable: Gets an existing variable with these parameters or create a new one.\n",
    "            counter_var = tf.get_variable(\"counter-\"+str(i), (), tf.int32, initializer=tf.zeros_initializer)\n",
    "            counter_vars.append(counter_var)\n",
    "            counter_adder_ops.append(\n",
    "                counter_var.assign_add(1, use_locking=True)\n",
    "            )\n",
    "\n",
    "    # create local version of parameters\n",
    "    for (i, device) in enumerate(devices):\n",
    "        with tf.device(device):\n",
    "            # tf.get_variable: Gets an existing variable with these parameters or create a new one.\n",
    "            local_param_var = tf.get_variable(\"local_param-\"+str(i), (), dtype, initializer=tf.zeros_initializer)\n",
    "            local_param_vars.append(local_param_var)\n",
    "\n",
    "            local_param_sync_op = local_param_var.assign(global_param_var)\n",
    "            local_param_sync_ops.append(local_param_sync_op)\n",
    "            \n",
    "            train_op = global_param_var.assign_add(1)\n",
    "            train_ops.append(train_op)\n",
    "\n",
    "\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    return (init_op, train_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_worker_threads(sess):\n",
    "    \"\"\"Creates a thread for each op in ops, running it iters times.\"\"\"\n",
    "\n",
    "    def barrier():\n",
    "        sess.run(counter_adder_ops[0])\n",
    "        while sess.run(counter_vars[0]) % num_workers != 0:\n",
    "            time.sleep(FLAGS.sleep_interval)\n",
    "        sess.run(counter_adder_ops[1])\n",
    "        \n",
    "        while sess.run(counter_vars[1]) % num_workers != 0:\n",
    "              time.sleep(FLAGS.sleep_interval)\n",
    "\n",
    "    def create_run_method(worker_id):\n",
    "        def _run():\n",
    "            local_param_var = local_param_vars[worker_id]\n",
    "            sync_op = local_param_sync_ops[worker_id]\n",
    "            train_op = train_ops[worker_id]\n",
    "            for i in range(FLAGS.iters):\n",
    "                barrier()\n",
    "                sess.run(sync_op)\n",
    "                barrier()\n",
    "                old_val, updated_val = sess.run([local_param_var, train_op])\n",
    "                print(\"\\nworker %2d, local_param %2d global_param %2d\" % (worker_id, old_val, updated_val))\n",
    "        \n",
    "        return _run\n",
    "\n",
    "    return [threading.Thread(target=create_run_method(i)) for i in range(num_workers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wait_for_threads_to_finish(threads):\n",
    "    while any(t.is_alive() for t in threads):\n",
    "        time.sleep(FLAGS.sleep_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_client():\n",
    "    tasks = [\"/job:worker/task:%d\"%(i) for i in range(num_workers)]\n",
    "\n",
    "    (init_op, add_ops) = create_graph(tasks)\n",
    "\n",
    "    # need tf.Session.reset if there are worker servers launched from before\n",
    "    # However, tf.Session.reset can hang if workers are in process of being\n",
    "    # brought up, hence more robust to do killall python\n",
    "    #  tf.Session.reset(\"grpc://\" + worker_ip)\n",
    "    \n",
    "    print(\"Creating session\")\n",
    "    sess = tf.Session(\"grpc://\" + worker_hosts[0],config=default_config())\n",
    "    sess.run(init_op)\n",
    "\n",
    "    worker_threads = create_worker_threads(sess)\n",
    "    [t.start() for t in worker_threads]\n",
    "    wait_for_threads_to_finish(worker_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating session\n",
      "\n",
      "worker  1, local_param  0 global_param  1\n",
      "\n",
      "worker  2, local_param  0 global_param  2\n",
      "\n",
      "worker  0, local_param  0 global_param  3\n",
      "\n",
      "worker  0, local_param  3 global_param  4\n",
      "\n",
      "worker  1, local_param  3 global_param  5\n",
      "\n",
      "worker  2, local_param  3 global_param  6\n",
      "\n",
      "worker  1, local_param  6 global_param  7\n",
      "\n",
      "worker  2, local_param  6 global_param  8\n",
      "\n",
      "worker  0, local_param  6 global_param  9\n",
      "\n",
      "worker  0, local_param  9 global_param 10\n",
      "\n",
      "worker  1, local_param  9 global_param 11\n",
      "\n",
      "worker  2, local_param  9 global_param 12\n",
      "\n",
      "worker  2, local_param 12 global_param 13\n",
      "\n",
      "worker  0, local_param 12 global_param 14\n",
      "\n",
      "worker  1, local_param 12 global_param 15\n",
      "\n",
      "worker  1, local_param 15 global_param 16\n",
      "\n",
      "worker  2, local_param 15 global_param 17\n",
      "\n",
      "worker  0, local_param 15 global_param 18\n",
      "\n",
      "worker  0, local_param 18 global_param 19\n",
      "\n",
      "worker  2, local_param 18 global_param 20\n",
      "\n",
      "worker  1, local_param 18 global_param 21\n",
      "\n",
      "worker  2, local_param 21 global_param 22\n",
      "\n",
      "worker  1, local_param 21 global_param 23\n",
      "\n",
      "worker  0, local_param 21 global_param 24\n",
      "\n",
      "worker  0, local_param 24 global_param 25\n",
      "\n",
      "worker  2, local_param 24 global_param 26\n",
      "\n",
      "worker  1, local_param 24 global_param 27\n",
      "\n",
      "worker  1, local_param 27 global_param 28\n",
      "\n",
      "worker  0, local_param 27 global_param 29\n",
      "\n",
      "worker  2, local_param 27 global_param 30\n"
     ]
    }
   ],
   "source": [
    "run_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
